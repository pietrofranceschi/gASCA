---
title: "ASCA Decomposition of Synthetic Count Data"
author: "Pietro Franceschi"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{ASCA Decomposition of Synthetic Count Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

To illustrate the use of ASCA in the analysis of count data we will considered a synthetic dataset which is available as a part of the package installation. This vignette will be also used to show how to use gg-style plotting to visualize the results of the ASCA analysis 

## The dataset

The synthetic data were ***

### Experimental design

* 10 count variables (identified as `s1 .. s10`)
* 2 factor design (5 time points, 3 treatments)
* 10 replicates

The example dataset was designed to have two latent variables for the time factor and one for the treatment.

Considering the 10 variables:

* the first four are responsive to the time factor. `s1` and `s2` mirrors LV1, `s3` and `s4` are linked to LV2.
* `s5` and `s6` are affected by the treatment factor.
* `s1` and `s2` are also sensitive to the interaction. In other words, for them the effect of treatment depends on time.
* The remaining variables contain only noise.

The dataset is included in the package as a two element list containing the count matrix `synth_count_data$counts` and a data frame holding the association between the 150 samples and the design factors (`synth_count_data$design`)

```{r}
## load the libraries
library(gASCA)
library(tidyverse)
library(ggrepel)

## load the dataset
data("synth_count_data")

## show a compact representation of the structure of the dataset
str(synth_count_data)

```


In this synthetic dataset we are dealing with a design with 5 time points and three treatments

```{r}
## time
unique(synth_count_data$design$time)

## treatment
unique(synth_count_data$design$treatment)

```

The following plot shows the distribution of the counts for variable `s1` as a function of the design factors

```{r fig.cap= "Illustration of the effects of the experimental design on the counts of Species #1"}
tibble(synth_count_data$design,
       data.frame(synth_count_data$counts)) %>% 
  ggplot() + 
  geom_point(aes(x = time, y = s1, fill = treatment),
             position = position_dodge(width = 0.4), pch = 21, size =3, alpha = 0.7) + 
  scale_fill_brewer(palette = "Set1", name = "Treatment") + 
  xlab("Time") + 
  theme_light() + 
  ggtitle("Variable S1") + 
  theme(aspect.ratio = 0.5, 
        legend.position = "bottom")

```

The effect of time on this variable is clear. Furthermore, _time_ and _treatment_ are also interacting as shown by the strong effect of treatment C at t4 

## Performing the decomposition

The function which performs the ASCA decomposition is `ASCA_decompose` and its results are organized in a list. To run the function it is necessary to provide:

-   a `matrix` or a `data.frame` with the counts;
-   a `data.frame` with the association between the samples and the design. Beware that numeric columns will results in a decomposition error!
-   a formula (actually a character vector) which specify the form of the decomposition;
-   a `list` with the parameters which will be passed to the `glm` call;
-   a character vector which specifies the type of errors which will be returned by `glm`;

The synthetic data we are dealing with are counts so the natural choice is to run a Poisson glm. The formula specifies that we are looking for a 3 term decomposition accounting for the effects of `time`, `treatment` and their interaction. Within `ASCA-decompose` the formula have to be specified following the standard R syntax. 

```{r}
## perform the decomposition
asca0 <- ASCA_decompose(
  d = synth_count_data$design,
  x = synth_count_data$counts, 
  f = "time + treatment + time:treatment",
  glm_par = list(family = poisson()))

```


Let's give a look to the structure of the `asca0` object


```{r}
str(asca0)

```

For a detailed description of the elements of the list the reader could refer to the documentation. 
The main output of the function is a list which holds the result of the decomposition (`asca0$decomposition`). Each matrix matrix contains the expected values estimated by the glm model of the individual variables. Printing the names helps understanding the structure of the array

```{r}
## matrix associated to the "time" factor
rownames(asca0$decomposition$time)
colnames(asca0$decomposition$time)
```

The 150 samples are organized along the first dimension, while the decomposition terms span the second dimension. The last dimension holds the experimental variables.

## Validation

A permutation based approach has been proposed as a test of the significance of the ASCA decomposition. The rationale behind the idea is to empirically construct a set of null models by repeatedly (and independently) permuting the design labels. These null models can then be used to construct the empirical distribution (and quantiles) of almost all model outcomes.

In the `gASCA` package the permutation based approach has been implemented. In particular it can be used to assess the significance of the following quantities 

* the L2 norm of the decomposition terms, which assess the significance of each model
* the variable importance
* the null distribution of the univariate pseudo-R2, which can be seen as a proxy of the overall model fit for each variable 

The `ASCA_permutation` function, requires as input the results of `ASCA_decompose` the number of independent permutations (500 by default) and the empirical quantile used to judge the significance (0.95 by default).

```{r}
## perform the permutation based approach
asca0_validation <- ASCA_permutation(asca0)
```

The following plot shows the results of the validation on the L2 norm of the three decomposition terms.

```{r}
asca0_validation$L2_qt %>% 
  as_tibble(rownames = "term") %>% 
  add_column(actual = asca0$terms_L2) %>% 
  mutate(term = factor(term, levels = term)) %>% 
  ggplot() + 
  geom_point(aes(x = term, y = actual), col = "steelblue", size = 3) +
  geom_segment(aes(x = term, y = 0, xend = term, yend = actual), col = "steelblue", lwd = 1) + 
  geom_point(aes(x = term, y = value), col = "darkred", size = 3) +
  ylab("term-L2") + xlab("Term") + 
  theme_light() + 
  theme(aspect.ratio = 0.5)
  
```

In the previous plot the term L2 norms (blue dots) are compared with their 95th percentile on the 500 permuted dataset (red dots). 
In all three cases, the observed matrix norm is larger than the one which should be expected by chance and this indicate that the three terms are all significant, as expected for the synthetic dataset.

In therms of variable importance the situation is the following:

```{r}
asca0$varimp %>% 
  as_tibble(rownames = "var") %>% 
  pivot_longer(-var) %>% 
  left_join(asca0_validation$varimp_qt %>% 
              as_tibble(rownames = "var") %>% 
              pivot_longer(-var, values_to = "value_qt")) %>% 
  mutate(var = factor(var, levels = unique(var))) %>% 
  mutate(name = factor(name, levels = unique(name))) %>% 
  ggplot() + 
  geom_point(aes(x = var, y = value), col = "steelblue", size = 3) +
  geom_segment(aes(x = var, y = 0, xend = var, yend = value), col = "steelblue", lwd = 1) + 
  geom_point(aes(x = var, y = value_qt), col = "darkred", size = 2, pch = 1) +
  facet_wrap(~name, scales = "free", nrow = 1) +
  ylab("Vecror Norm") + xlab("Variable") + 
  theme_light() + 
  theme(aspect.ratio = 0.5)
```

As before, the red dots indicate the empirical quantiles upon permutation of the design. The picture we get with this analysis is in line to the characteristics of the simulated dataset. In particular.
 
 * As expected the first four variables are responsive to a different extent to the time factor
 * S5 and s6 are correctly identified as sensitive to treatment. Unexpectedly, however, also s1 turns out to be responsive to treatment, but remember that s1 was affected by the interaction between the two main factors. 
 * As far as the interaction is concerned, only s1 seems to be significantly affected, with s2 

### Univariate model fit

The last parameter which was included in the validation process is the pseudo-R2 of the set of univariate models. In classical ASCA, the inspection and the eigen decomposition of the residual matrix has proven to be useful and informative in identifying unmodeled patterns in the data. Unfortunately, this step it is not necessarily informative in glm based ASCA, since the matrix of residuals in the response space is not always easy to interpret (e.g  the error matrix of a poisson ASCA of a matrix of species counts holds the difference between observed and predicted counts).  
Here we propose to use a measure of univariate model fit at least to identify the set of variables which are less responsive to the experimental design. For glms, pseudo-$R^{2}$ is one of the possible measures of fit which is defined comparing the devicence of the fit of the proposed model with the one of a "null" constant model.

$$
R^{2}_{pseudo} = 1 - \frac{deviance_{residuals}}{deviance_{null}}
$$

In practical terms, the parameter gets closer to 1 if the deviance of the residuals of the proposed model is smaller than the one of a constant model.  

The "significance" of this parameter can also be assessed in the permutation test and the validation is included in the package functionality.
In the package, the calculation of the pseudo-$R^{2}$ for the set of the measured variables is part of the output of the `ASCA_decompose` function. In the case of the synthetic dataset the trend of the pseudo_$R^{2}$ for the the variables and the results of the validation are shown in the following plot: 

```{r}
asca0_validation$R2_qt %>% 
  as_tibble(rownames = "var") %>%  
  add_column(actual = asca0$pseudoR2) %>% 
  mutate(var = factor(var, levels = unique(var))) %>% 
  ggplot() + 
  geom_point(aes(x = var, y = actual), col = "steelblue", size = 3) +
  geom_segment(aes(x = var, y = 0, xend = var, yend = actual), col = "steelblue", lwd = 1) + 
  geom_point(aes(x = var, y = value), col = "darkred", size = 3, pch = 1) +
  ylab("pseudo-R2") + xlab("Variable") + 
  theme_light() + 
  theme(aspect.ratio = 0.5)
  
```

Also here the blue dots represent the univariate model pseudo-$R^{2}$, while the red dots shows its null 95th percentile. The first 6 variables shows a large and significant pseudo-$R^{2}$ and this is not only indicating that they are significantly responding to the design, but also that the proposed model is capturing a large fraction of the data variability. It is also interesting to observe that, not unexpectedly, the univariate null quantiles are similar for all the 10 variables.

It is important to point out that when dealing with a glm with an identity link function, the overall model fit can still be inspected relying on the matrix of the residuals in the response space. As in classical ASCA, a PCA of this matrix is useful to perform a multivariate investigation of the unmodeled structure of the dataset. Bear in mind that in the case of dataset consisting of reasonably large counts, a gaussian modelling of the log-transformed values could give a good representation of the data structure. 


## Analysis of the decomposition terms

### Eigenvectors and Multivariate Decomposition

Let's now analyse the three decomposition matrices in terms of their multivariate latent factors. Depending on the term these LV can be interpreted as "eigentrends" (for time) or "eigentreatment" (for treatment) and they are actually what we would like to interpret. Remember that the rank of the matrix associated to each factor is determined by the number of levels of each factor because the decomposition matrices are constructed with the model expected values. The number of LVs will be then equal to the number of factors minus one.

As an example, this is the set of predicted values for the time factor of variable s1 

```{r}
plot(asca0$decomposition$time[,"s1"], ylab = "I", xlab = "Sample Index", pch = 19, col = "steelblue")
```

The presence of a limited number of constant values is coherent with the levels of the time factor, while the scale on the y axis 
shows that predictions are performed in the linear predictor space.  Since the _time_ factor has 5 levels the rank of the matrix will be four. We then expect four eigentrends. Their relative importance is proportional to the individual eigenvalues.


The SVD decomposition of the ASCA array is performed by the `ASCA_svd` function. 

```{r}
asca0_svd <- ASCA_svd(asca0$decomposition)
```

Also here the result is a list with each element holding the results of the svd of the single decomposition terms which have been performed by using the `prcomp` function of base R. As in a usual PCA analysis the amount of explained variability (square of the standard deviation) can be used to identify how many eigentrends it would be necessary to consider to present an informative representation of the overall matrix. This type of representation is known as *scree plot*


```{r}
## here we make a scree plot of the three decomposition terms
map(asca0_svd, ~ .x$sdev %>% as_tibble(., rownames = "PC")) %>% 
  enframe(name = "term", value = "data") %>%
  # mutate(data = map(data,~ .x %>% mutate(value = (value/sum(value))*100))) %>% 
  unnest(cols = c(data)) %>% 
  mutate(PC = factor(PC, levels = unique(PC))) %>% 
  ggplot() + 
  geom_col(aes(x = PC, y = value^2), fill = "steelblue") + 
  facet_wrap(~term, scales = "free", ncol = 3) + 
  ylab("Variances") + 
  theme_light() + 
  theme(aspect.ratio = 0.5)
```

The previous plot suggests that two LVs represent good aproximation for all the three factors (actually for treatment 2 LVs are not an approximation)

Let's start with the `time` term. Its latent variables can be interpreted as latent "eigentrend" which highlights underlying general patterns shared by group of variables. The best way to highlight that is to plot the separate scores for the first two LVs as a function of the *time* design factor


```{r}
asca0_svd$time$x %>% 
  as_tibble() %>% 
  mutate(across(everything(),~round(.x,2))) %>%      ## this is needed to compensate for computational inaccuracies
  add_column(time = synth_count_data$design$time) %>% 
  unique() %>% 
  select(time, PC1, PC2) %>% 
  pivot_longer(starts_with("PC")) %>% 
  ggplot() + 
  geom_point(aes(x = time, y = value), col = "steelblue", alpha = 0.7, size = 3) + 
  geom_line(aes(x = time, y = value, group = name),col = "steelblue", alpha = 0.7) + 
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  facet_wrap(~name, ncol = 2, scales = "free") + 
  theme_light() + ylab("Scores") +
  theme(aspect.ratio = 0.4)
```

The first eigentrend is characterized by a steadily increasing tendency which contrasts with the evolution of the second "U" shaped profile. As in standard pca, their loadings will tell which measured variables are mainly associated to them 

```{r}
asca0_svd$time$rotation %>% 
  as_tibble(rownames = "species") %>% 
  mutate(species = factor(species, levels = paste0("s",1:10))) %>% 
  select(species,PC1,PC2) %>% 
  pivot_longer(starts_with("PC")) %>% 
  ggplot() + 
  geom_point(aes(x = species, y = value), col = "steelblue", size = 3) +
  geom_segment(aes(x = species, y = 0, xend = species, yend = value), col = "steelblue", lwd = 1) + 
  facet_wrap(~name, ncol = 1) + 
  theme_light() + ylab ("Loadings")+
   theme(aspect.ratio = 0.4)
```

The loadings plot indicates that s1 and s1 are highly contributing to the increasing eigentrend, while s3 and s4 mainly characterize the second LV. This is in keeping with the expected characteristics of the synthetic dataset. The real time trends of the measured values for s1 amd s3 are indeed the following.

```{r}
tibble(synth_count_data$design,
       data.frame(synth_count_data$counts)) %>% 
  select(time, treatment, s1,s3) %>% 
  pivot_longer(starts_with("s")) %>% 
  ggplot() + 
  geom_point(aes(x = time, y = value, fill = treatment),
             position = position_dodge(width = 0.4), pch = 21, size =2, alpha = 0.7) + 
  scale_fill_brewer(palette = "Set1", name = "Treatment") + 
  facet_wrap(~name, ncol = 2, scales = "free") +
  xlab("Time") + 
  theme_light() + 
  theme(aspect.ratio = 0.5, 
        legend.position = "bottom")

```

In the case of the `treatment`, we are dealing with only two eigenvectors since the number of levels of this factor is three.


Due to the limited number of levels the plots of the individual eigen-treatments highligths a differantiation between the three levels along PC1, and a separation between A/C and B along PC2.

```{r}

asca0_svd$treatment$x %>% 
  as_tibble() %>% 
  mutate(across(everything(),~round(.x,2))) %>%      ## this is needed to compensate for computational differences across the glm estimations
  add_column(treatment = synth_count_data$design$treatment) %>% 
  unique() %>% 
  select(treatment, PC1, PC2) %>% 
  pivot_longer(starts_with("PC")) %>% 
  ggplot() + 
  geom_point(aes(x = treatment, y = value), col = "steelblue", alpha = 0.7, size = 3) + 
  geom_line(aes(x = treatment, y = value, group = name),col = "steelblue", alpha = 0.7) + 
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  facet_wrap(~name, ncol = 1, scales = "free") + 
  theme_light() + ylab("Scores in response space")+
  theme(aspect.ratio = 0.4)

```

The same line of reasoning can be applied also to the interpretation of the LVs of the `time:treatment` factors, but the interpretation of the LV associated to the interaction terms is not straightforward. Combining two or more decomposition terms prior to SVD could be an informative workaround to the problem. In particular, combining the `time` and `time:treatment` could highlight the role of the treatment in shaping the time trends.  

### Combining terms

The function `ASCA_combine_terms` allows the combination of different factor matrices. The function accepts as input the output of an `ASCA_decomposition` and a character vector specifying which terms should be combined.

```{r}
## combine two terms
asca0_comb <- ASCA_combine_terms(asca0, c("time","time:treatment"))

## perform the svd of the combined decomposition 
asca0_comb_svd <- ASCA_svd(asca0_comb$decomposition)
```

In terms of the eigenvalues of the LVs:

```{r}
## here we use the base pscree plot
plot(asca0_comb_svd$`time+time:treatment`, main = "time + time:treatment", col = "steelblue")
```

The rank of the matrix here is larger (10), but the major trends are still captured by the first two components. Their eigentrends are plot in the following figure: 

```{r}
asca0_comb_svd$`time+time:treatment`$x %>% 
  as_tibble() %>% 
  mutate(across(everything(),~round(.x,2))) %>%      ## this is needed to compensate for computational inaccuracies
  add_column(time = synth_count_data$design$time,
             treatment = synth_count_data$design$treatment) %>% 
  unique() %>% 
  select(time,treatment, PC1, PC2) %>% 
  pivot_longer(starts_with("PC")) %>% 
  ggplot() + 
  geom_point(aes(x = time, y = value, col = treatment), alpha = 0.7, size = 3) + 
  geom_line(aes(x = time, y = value, col = treatment, group = interaction(name,treatment)), alpha = 0.7) + 
  geom_hline(yintercept = 0, col = "red", lty = 2) +
  facet_wrap(~name, ncol = 2, scales = "free") + 
  theme_light() + 
  theme(aspect.ratio = 0.4)
```

Here the time dependent interplay between time and treatment along the first component is clear. Treatment "C" is responsible of a modification in the time profile of the first eigentrend. In terms of loadings, the picture is similar to the previous one (the flip in the sign of the loadings of eig2 is the result of the sign indetermination for svd). If one focusses on eig1 (which accounts for almost the 90% of the matrix variance), s1 and s2 are almost the only contributors and this highlights the influence of `time` and `time:treatment` on the measured intensities of these two variables.


```{r}
asca0_comb_svd$`time+time:treatment`$rotation %>% 
  as_tibble(rownames = "species") %>% 
  mutate(species = factor(species, levels = paste0("s",1:10))) %>% 
  select(species,PC1,PC2) %>% 
  pivot_longer(starts_with("PC")) %>% 
  ggplot() + 
  geom_point(aes(x = species, y = value), col = "steelblue", size = 3) +
  geom_segment(aes(x = species, y = 0, xend = species, yend = value), col = "steelblue", lwd = 1) + 
  facet_wrap(~name, ncol = 1) + 
  theme_light() + ylab ("Loadings")+
   theme(aspect.ratio = 0.4)
```

The role of S1 in determining eig1 can be seen in the original data after log transformation.

```{r }
tibble(synth_count_data$design,
       data.frame(synth_count_data$counts)) %>% 
  mutate(across(starts_with("s"), ~log(.x))) %>% 
  select(time,treatment,s1) %>% 
  filter(is.finite(s1)) %>% 
  ggplot() + 
  geom_point(aes(x = time, y = s1, fill = treatment),
             position = position_dodge(width = 0.4), pch = 21, size =3, alpha = 0.7) + 
  scale_fill_brewer(palette = "Set1", name = "Treatment") + 
  xlab("Time") + 
  theme_light() + 
  ggtitle("Variable S1") + 
  theme(aspect.ratio = 0.5, 
        legend.position = "bottom")

```

Here the green dots of treatment C are absent in t1 (so C is "negatively" contributing at t1), while they get larger than their blue and red counterparts in t3, t4 and - marginally- t5. Treatment C has then a steeper increasing trend, as highlighted by the first eigen-trend of the combined ASCA.



